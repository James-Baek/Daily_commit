import urllib.request
from bs4 import BeautifulSoup

# 뷰티풀 숩 임포트


########################################################
######### 네이버 실시간 검색어 가져오는 예제 ##############
########################################################

def main():
    
    # URL 데이터를 가져올 사이트 url 입력
    url = "http://www.naver.com"
        
    # URL 주소에 있는 HTML 코드를 soup에 저장합니다.
    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")
    
    
    list = []

    for naver_text in soup.find_all("ul", class_="ah_l"):
        for t in naver_text.find_all("span", class_="ah_k"):
            list.append(t.get_text())

    print(list)


if __name__ == "__main__":
    main()
    


########################################################
######### 기사 제목 & 내용 가져오는 예제 ##############
########################################################

import urllib.request
from bs4 import BeautifulSoup



def main():
    
    
    # URL 데이터를 가져올 사이트 url 입력
    url = "http://www.kyeonggi.com/?mod=news&act=articleList&view_type=S&sc_code=1439458030"

    # URL 주소에 있는 HTML 코드를 soup에 저장합니다.
    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")


    ##### list_title과, list_content에 append()를 사용하여 원하는 정보를 하나씩 담아 출력합니다. #####

    list_title = []
    list_content = []

    for news_title in soup.find_all("div", class_="list-titles"):
        list_title.append(news_title.get_text())

    for news_content in soup.find_all("p", class_="list-summary") :

        list_content.append(news_content.get_text())

    print(list_title)
    print(list_content)

if __name__ == "__main__":
    main()
    

########################################################
######### 링크 끝에 숫자 바뀌는거 맞춰서 크롤링 예제#######
########################################################

import urllib.request
from bs4 import BeautifulSoup



def main():
    list_pagination = []

    for i in range(0, 5):
        url = "http://sports.donga.com/Enter?p="+str(20*i+1)+"&c=02"
        source_code = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(source_code, "html.parser")
        
        
        for i in soup.find_all("div", class_="list_content "):
            for j in i.find_all("span", class_="tit"):
                list_pagination.append(j.get_text())

    print(list_pagination)


if __name__ == "__main__":
    main()


########################################################
################## href 수집 예제 ######################
########################################################


import urllib.request
from bs4 import BeautifulSoup



def main():
    print("href 출력해보기")

    list_href = []

    url = "http://sports.donga.com/Enter"
    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")

    # 반복문을 사용해 원하는 정보 range(3,23)까지 find("a")["href"] 를 사용해서
    # href 모두 수집하여 list_href에 저장
    

    for i in range(3, 23):
        list_href.append(soup.find_all("span", class_="tit")[i].find("a")['href'])

    print(list_href)


if __name__ == "__main__":
    main()
    
    
########################################################
########크롤링한 링크 주소에 있는 내용을 크롤링############
########################################################    
    
    
import urllib.request
from bs4 import BeautifulSoup



def main():
    list_href = []
    list_content = []

    url = "https://news.sbs.co.kr/news/newsflash.do?plink=GNB&cooper=SBSNEWS"
    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")

    for href in soup.find_all("div", class_="mfn_inner"):
        list_href.append("https://news.sbs.co.kr"+href.find('a')['href'])

    for i in range(0, len(list_href)):
        url = list_href[i]
        source_code = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(source_code, "html.parser")

        list_content.append(soup.find('div', class_="text_area").get_text())

    print(list_href)
    print(list_content)


if __name__ == "__main__":
    main()
    
    
########################################################
############언론사 기사 내용에서 키워드 찾기###############
########################################################    


    
import urllib.request
from bs4 import BeautifulSoup



def main():
    list = []
    list_content = []

    url = "https://news.sbs.co.kr/news/newsflash.do?plink=GNB&cooper=SBSNEWS"
    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")

    for i in soup.find_all("div", class_="mfn_inner"):
        list.append('https://news.sbs.co.kr'+i.find('a')['href'])

    for i in range(0, len(list)):
        url = list[i]
        source_code = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(source_code, "html.parser")

        list_content.append(soup.find('div', class_='text_area').get_text())

    for i in list_content:

        if "국회" in i:
            print("Okay")
        else:
            print("No")


if __name__ == "__main__":
    main()
    
########################################################
################뉴스 기사 링크 수집하기###################
########################################################        

import urllib.request
from bs4 import BeautifulSoup



def main():
    print("최신 뉴스 기사 href 수집")

    # href 수집할 사이트 주소 입력
    url = "https://news.nate.com/recent?mid=n0100"

    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")

    # 1. a 태그가 있는 div 태그 및 class 찾기
    # 2. find("a")["href"]로 href 추출
    # 3. "//"로 시작하는 링크는 앞에 "http:"를 추가하여
    #    완전한 링크 만들기

    for i in soup.find_all("div", class_="mduSubjectList"):
        print("https:"+i.find('a')['href'])


if __name__ == "__main__":
    main()
    
    
########################################################
######수집한 링크에 들어가 웹 페이지 내용 수집하기#########
########################################################       
    
import urllib.request
from bs4 import BeautifulSoup



def main():
    print("최신 뉴스 기사 href 안의 내용 수집")

    # href 수집할 사이트 주소 입력
    url = "https://news.nate.com/recent?mid=n0100"

    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")

    # 1. a 태그가 있는 div 태그 및 class 찾기
    # 2. find("a")["href"]로 href 추출
    # 3. "//"로 시작하는 링크는 앞에 "http:"를 추가하여
    #    완전한 링크 만들기
    # 4. list에 넣기

    links = []

    for i in soup.find_all("div", class_="mduSubjectList"):
        links.append("https:"+i.find('a')['href'])

    # 1. list의 크기 만큼 for 루프 출력
    # 2. url에 list에 있는 href 링크 하나씩 넣어서 출력하기

    for i in links:
        url = i
        source_code = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(source_code, "html.parser")

        print(soup)


if __name__ == "__main__":
    main()
    

########################################################
#############크롤링해온 텍스트의 문자열 가공###############
########################################################    

import urllib.request
from bs4 import BeautifulSoup


def main():
    url = "http://www.newsis.com/eco/list/?cid=10400&scid=10404"
    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")

    for text in soup.find_all("strong",class_="title"):

        num = text.get_text().find("…")

        if num is not 0:
            print(text.get_text()[:num], end='')
            print(text.get_text()[num+3:])
        else:
            print(text.get_text())


if __name__ == "__main__":
    main()
    


########################################################
####################정규식 알아보기######################
########################################################  

import re


def main():
    sentence = "제보는 032-232-3245 또는 010-222-2233 으로 연락주시기 바랍니다."

    compile_text = re.compile(r'\d\d\d-\d\d\d-\d\d\d\d')
    match_text = compile_text.findall(sentence)
    print(match_text)

    email = ["aa2@naver.com", "kbc@aaaaa", "Alice@Alice.com", "ILove@CodingLove"]

    compile_text = re.compile('^[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$')

    for i in email:
        print(compile_text.match(i) != None)


if __name__ == "__main__":
    main()


########################################################
##################엑셀 시트로 저장하기####################
########################################################  
    
import urllib.request

from bs4 import BeautifulSoup
from openpyxl import Workbook
from elice_utils import EliceUtils

elice_utils = EliceUtils()


def main():
    wb = Workbook()

    sheet1 = wb.active
    file_name = 'sample.xlsx'

    url = "http://www.newsis.com/eco/list/?cid=10400&scid=10404"
    source_code = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(source_code, "html.parser")

    articles = []

    for i in soup.find_all("strong", class_="title"):
        articles.append(i.get_text())
    print(articles)
    for row_index in range(1, len(articles)+1):
        sheet1.cell(row=row_index, column=1).value = row_index
        sheet1.cell(row=row_index, column=2).value = articles[row_index - 1]

    wb.save(filename=file_name)
    elice_utils.send_file(file_name)


if __name__ == "__main__":
    main()
    
    
